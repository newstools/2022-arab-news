LONDON: Microsoft revealed on Tuesday that companies using its facial recognition technology are no longer permitted to do so for things such as identifying emotions, gender or age. As part of its new “responsible AI standard,” Microsoft’s overhaul of its artificial intelligence ethics policy is intended to keep “people and their goals at the center of system design decisions.” “We collaborated with internal and external researchers to understand the limitations and potential benefits of this technology and navigate the tradeoffs,” said Sarah Bird, a product manager at Microsoft. “In the case of emotion classification specifically, these efforts raised important questions about privacy, the lack of consensus on a definition of ‘emotions,’ and the inability to generalize the linkage between facial expression and emotional state across use cases.” What this means is that Microsoft will limit access to some features of its facial recognition services — known as Azure Face — and remove others entirely. Users or companies wishing to use this service will have to apply to use Azure Face for facial identification and tell Microsoft exactly how and where they will be deploying its systems. They will also need to prove that they are matching Microsoft’s AI ethics standards and that the features benefit the end-user and society. Microsoft says that even companies that are granted access will no longer be able to use some of the more controversial features of Azure Face, including tools aimed at detecting gender, age, smile, hair, as well as other emotional states and attributes. Additionally, Microsoft is limiting the use of its custom neural voice technology, used to create AI voices based on recordings of real people, known as deepfakes. While the company is retiring those features, Microsoft will still use some of the tools in certain products such as “Seeing AI” app, which uses machine vision to verbally describe the world for users with vision problems.